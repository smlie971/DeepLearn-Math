# 导入必要的库
import numpy as np

# 定义目标函数
def f(x, y):
    return x**2 + y**2

# 定义梯度函数
def gradient(x, y):
    return np.array([2*x, 2*y])

# 梯度下降算法
def gradient_descent(initial_x, initial_y, learning_rate, num_iterations):
    # 初始化变量
    x, y = initial_x, initial_y
    for i in range(num_iterations):
        # 计算当前点的梯度
        grad = gradient(x, y)
        # 更新变量
        x -= learning_rate * grad[0]
        y -= learning_rate * grad[1]
        # 打印当前迭代信息
        print(f"Iteration {i+1}: x={x:.4f}, y={y:.4f}, f(x,y)={f(x, y):.4f}")
    return x, y

# 设置初始值、学习率和迭代次数
initial_x = 10.0  # 初始x值
initial_y = 10.0  # 初始y值
learning_rate = 0.1  # 学习率
num_iterations = 50  # 迭代次数

# 运行梯度下降算法
optimal_x, optimal_y = gradient_descent(initial_x, initial_y, learning_rate, num_iterations)
print(f"Optimal values: x={optimal_x:.4f}, y={optimal_y:.4f}, f(x,y)={f(optimal_x, optimal_y):.4f}")
