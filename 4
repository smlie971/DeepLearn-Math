import numpy as np

# 设置随机种子以确保结果可复现
np.random.seed(42)

# 生成随机数据集
X = np.random.rand(64, 12)  # 64个样本，每个样本12个像素
y = np.random.randint(2, size=64)  # 64个标签，值为0或1

# 将标签转换为独热编码
y_one_hot = np.eye(2)[y]

# 神经网络结构
input_size = 12  # 输入层大小
hidden_size = 3  # 隐藏层大小
output_size = 2  # 输出层大小

# 随机初始化权重和偏置
w_2_1n = np.random.randn(input_size, hidden_size) * 0.01
b_2_1 = np.zeros(hidden_size)
w_2_2n = np.random.randn(hidden_size, output_size) * 0.01
b_2_2 = np.zeros(output_size)

# 激活函数及其导数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# 均方误差损失函数
def mse_loss(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

# 训练过程
def train(X, y, epochs, learning_rate, w_2_1n, b_2_1, w_2_2n, b_2_2):
    for epoch in range(epochs):
        # 正向传播
        z_2_n = np.dot(X, w_2_1n) + b_2_1
        hidden_output = sigmoid(z_2_n)
        
        z_3_n = np.dot(hidden_output, w_2_2n) + b_2_2
        output = sigmoid(z_3_n)
        
        # 计算损失
        loss = mse_loss(y, output)
        
        # 反向传播
        # 输出层误差
        output_errors = y - output
        output_delta = output_errors * sigmoid_derivative(output)
        
        # 隐藏层误差
        hidden_errors = np.dot(output_delta, w_2_2n.T)
        hidden_delta = hidden_errors * sigmoid_derivative(hidden_output)
        
        # 更新权重和偏置
        w_2_2n += learning_rate * np.dot(hidden_output.T, output_delta)
        b_2_2 += learning_rate * np.sum(output_delta, axis=0)
        
        w_2_1n += learning_rate * np.dot(X.T, hidden_delta)
        b_2_1 += learning_rate * np.sum(hidden_delta, axis=0)
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss}")

    return w_2_1n, b_2_1, w_2_2n, b_2_2
print("输入层到隐藏层的权重：", w_2_1n)
print("隐藏层的偏置：", b_2_1)
print("隐藏层到输出层的权重：", w_2_2n)
print("输出层的偏置：", b_2_2)
